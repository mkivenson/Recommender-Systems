---
title: "Career Recommender System"
author: "Mary Anna Kivenson"
date: "July 13, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, message=FALSE, warning=FALSE}
library(rvest)
library(tidyverse)
library(magrittr)
library(RCurl)
library(bigrquery)
library(stringr)
```


## Function to Scrape Job Postings
This function `get_jobs` accepts a list of job titles and returns a dataframe containing company, location, summary, link, and description for the first 10 pages of Indeed job postings for each title.

```{r get_jobs, include=FALSE}
get_jobs <- function(job_title){
  
  #create empty dataframe containing columns for company, location, summary, link, and description
  listings <- data.frame(title=character(),
                 company=character(), 
                 location=character(), 
                 summary=character(), 
                 link=character(), 
                 description = character(),
                 stringsAsFactors=FALSE) 
  listings_all <- data.frame()
  for (job_title in job_list){
  #get the first 3 pages of job listings for the inputted job title
    for (i in seq(0, 30, 10)){
      url_ds <- paste0('https://www.indeed.com/jobs?q=',job_title,'&l=all&start=',i)
      var <- read_html(url_ds)
      
      #job title
      title <-  var %>% 
        html_nodes('#resultsCol .jobtitle') %>%
        html_text() %>%
        str_extract("(\\w+.+)+") 
      
      #company
      company <- var %>% 
        html_nodes('#resultsCol .company') %>%
        html_text() %>%
        str_extract("(\\w+).+") 
      
      #location
      location <- var %>%
        html_nodes('#resultsCol .location') %>%
        html_text() %>%
        str_extract("(\\w+.)+,.[A-Z]{2}")   
      #summary
      summary <- var %>%
        html_nodes('#resultsCol .summary') %>%
        html_text() %>%
        str_extract(".+")
      
      #link
      link <- var %>%
        html_nodes('#resultsCol .jobtitle .turnstileLink, #resultsCol a.jobtitle') %>%
        html_attr('href') 
      link <- paste0("https://www.indeed.com",link)
        
      listings <- rbind(listings, as.data.frame(cbind(title,
                                                      company,
                                                      location,
                                                      summary,
                                                      link)))
    }
    
    listings_all <- rbind(listings_all, listings)
  }
  #remove duplicate links
  listings_all %<>%
    distinct(link, .keep_all = TRUE)
  
  #obtain full description for all job postings
  for (i in (1:length(listings_all$link))){
    desciption <- tryCatch(
       html_text(html_node(read_html(as.character(listings_all$link[i])),'.jobsearch-jobDescriptionText')),
       error=function(e){NA}
    )
    if (is.null(desciption)){
      desc <- NA
    }
    listings_all$description[i] <- desciption
  }
  
  #remove duplicate descriptions
  listings_all %<>%
    distinct(description, .keep_all = TRUE)
  
  return(listings_all)
}
```


## Apply Scraping Function

This code junk creates a list of tech-related jobs in `job_list`. This list is then used as an input in the `get_jobs` function. The output of this function is shown in the dataframe below.
```{r get_jobs use, eval=FALSE, include=TRUE}
job_list <- list('Data+Scientist', 'Data+Analyst', 'Business+Analyst', 'Computer+Scientist', 'Software+Engineer', 'Systems+Analyst', 'Web+Developer', 'Database+Administrator', 'Programmer', 'SEO+Consultant', 'Digital+Marketing', 'UX+Designer', 'UI+Designer', 'Front+End+Developer', 'Data+Architect', 'Data+Engineer', 'Quality+Assurance+Analyst', 'Statistician', 'Software+Tester', 'Product+Analyst', 'IT+Support', 'Game+Developer', 'CRM+Analyst', 'Network+Engineer', 'Data+Modeler')
job_table <- get_jobs(job_list)
job_table$description = str_replace_all(job_table$description, pattern = "\\n", replacement = " ")
job_table$insert_date <- as.Date(Sys.time(), tz = "UTC")
head(job_table)
```


## Function to Store Job Postings

This function stores the previous dataframe into a Google BigQuery datatable.
```{r store_bigquery, eval=FALSE, include=TRUE}
store_jobs <- function(datatable = job_table){
  bq_auth(path = "deep-rigging-245523-13178f463b29.json")
  bq_perform_upload("deep-rigging-245523", "Projects", "job_listings", datatable, create_disposition = "CREATE_IF_NEEDED")  
}
store_jobs()
```